{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5171d9c3",
   "metadata": {},
   "source": [
    "### Installing and Importing Required Libraries\n",
    "\n",
    "We begin by installing all the libraries needed for our protein function prediction project. This includes tools for biological data handling (`biopython`, `goatools`), machine learning (`scikit-learn`, `scikit-multilearn`), data manipulation (`pandas`, `numpy`), and visualization (`matplotlib`).\n",
    "\n",
    "After installation, we import these libraries to make them available in our environment. This setup ensures we can preprocess biological data, perform multi-label classification, manage the Gene Ontology hierarchy, and evaluate our models using metrics like F1-score and precision. We've also included utilities like `os` to handle file paths when needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac3b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Install all required libraries\n",
    "!pip install biopython\n",
    "!pip install goatools\n",
    "!pip install scikit-learn\n",
    "!pip install scikit-multilearn\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib  # Optional, for any future plotting\n",
    "\n",
    "# ✅ Now you can safely import everything used in your notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # Optional if needed\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.decomposition import PCA  # If used later\n",
    "from sklearn.metrics import f1_score, precision_score  # For evaluation\n",
    "\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "from Bio import SeqIO\n",
    "from goatools.obo_parser import GODag\n",
    "\n",
    "import os  # For managing paths if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfab209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the embeddings from a CSV file\n",
    "embeddings = pd.read_csv('embeddings.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d8ad65",
   "metadata": {},
   "source": [
    "###  Loading Protein Sequences, Annotations, and GO Hierarchy\n",
    "\n",
    "We load three essential datasets required for our project:\n",
    "\n",
    "1. **Protein Sequences**: Using `Bio.SeqIO`, we parse the `train_sequences.fasta` file and store each protein’s ID and its amino acid sequence in a dictionary called `train_sequences`.\n",
    "\n",
    "2. **GO Annotations**: We read the `train_terms.tsv` file into a DataFrame called `train_terms`. This contains mappings between protein IDs and their associated Gene Ontology (GO) terms.\n",
    "\n",
    "3. **GO Hierarchy**: Using `goatools`, we parse the `go-basic.obo` file with `GODag`, which gives us access to the hierarchical structure of GO terms. This will be useful later when incorporating ontology-aware strategies like ancestor term propagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fc9e79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\OneDrive - uniroma1.it\\Esami\\Statistical Learning\\Project\\Implementation\\Train\\go-basic.obo: fmt(1.2) rel(2023-01-01) 46,739 Terms\n"
     ]
    }
   ],
   "source": [
    "# Load train_sequences.fasta\n",
    "from Bio import SeqIO\n",
    "train_sequences = {}\n",
    "for record in SeqIO.parse(r'C:\\Users\\Utente\\OneDrive - uniroma1.it\\Esami\\Statistical Learning\\Project\\Implementation\\Train\\train_sequences.fasta', 'fasta'):\n",
    "    train_sequences[record.id] = str(record.seq)\n",
    "# Load train_terms.tsv\n",
    "train_terms = pd.read_csv(r'C:\\Users\\Utente\\OneDrive - uniroma1.it\\Esami\\Statistical Learning\\Project\\Implementation\\Train\\train_terms.tsv', sep='\\t')\n",
    "# Load go-basic.obo\n",
    "from goatools.obo_parser import GODag\n",
    "go_dag = GODag(r'C:\\Users\\Utente\\OneDrive - uniroma1.it\\Esami\\Statistical Learning\\Project\\Implementation\\Train\\go-basic.obo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32d404",
   "metadata": {},
   "source": [
    "###  Building the Target Label Matrix\n",
    "\n",
    "We construct the binary label matrix needed for multi-label classification by following these steps:\n",
    "\n",
    "1. **Top GO Terms Selection**: We focus on the 1500 most frequent GO terms across all proteins, helping reduce label sparsity and memory usage.\n",
    "\n",
    "2. **Filtering Proteins**: We ensure we only retain annotations for proteins that have corresponding embeddings available.\n",
    "\n",
    "3. **Term Grouping**: For each protein, we collect its list of GO terms and fill in empty lists for proteins with no annotations in the top 1500 terms.\n",
    "\n",
    "4. **Binarization**: Using `MultiLabelBinarizer`, we convert each protein’s list of GO terms into a binary row vector — 1 if the term is present, 0 otherwise.\n",
    "\n",
    "5. **Final Label Matrix**: We assemble everything into a clean DataFrame (`label_df`) with one row per protein and one column per GO term. This matrix is then saved for reuse.\n",
    "\n",
    "This matrix will serve as our target `Y` for training models and evaluating predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423358df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Label matrix shape: (142246, 1501)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "k = 1500  # Number of top GO terms to keep\n",
    "\n",
    "protein_ids = embedding_df[\"Protein Id\"].tolist()\n",
    "\n",
    "# === Step 2: Load GO Annotations ===\n",
    "terms_df = pd.read_csv(\"Train/train_terms.tsv\", sep='\\t')\n",
    "\n",
    "# === Step 3: Filter to proteins that have embeddings ===\n",
    "terms_df = terms_df[terms_df['EntryID'].isin(protein_ids)]\n",
    "\n",
    "# === Step 4: Keep only the top 1500 GO terms based on frequency ===\n",
    "top_terms = terms_df['term'].value_counts().nlargest(k).index\n",
    "terms_df = terms_df[terms_df['term'].isin(top_terms)]\n",
    "\n",
    "# === Step 5: Group GO terms by protein ===\n",
    "grouped_terms = terms_df.groupby(\"EntryID\")[\"term\"].apply(list)\n",
    "\n",
    "# === Step 6: Fill missing proteins with empty lists ===\n",
    "go_terms_list = [grouped_terms.get(pid, []) for pid in protein_ids]\n",
    "\n",
    "# === Step 7: Create binary matrix ===\n",
    "mlb = MultiLabelBinarizer(classes=sorted(top_terms))\n",
    "Y = mlb.fit_transform(go_terms_list)\n",
    "go_terms = mlb.classes_\n",
    "\n",
    "# === Step 8: Turn into a DataFrame ===\n",
    "label_df = pd.DataFrame(Y, columns=go_terms)\n",
    "label_df.insert(0, \"Protein Id\", protein_ids)\n",
    "\n",
    "# === (Optional) Save for reuse ===\n",
    "label_df.to_csv(\"label_matrix_1500.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Label matrix shape: {label_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f27aaf",
   "metadata": {},
   "source": [
    "###  Retrieving GO Terms for a Protein\n",
    "\n",
    "We define a helper function `get_go_terms(protein_id)` to easily retrieve the GO terms associated with a given protein:\n",
    "\n",
    "- It searches the `label_df` (our binary label matrix) for the specified `protein_id`.\n",
    "- If the protein exists, it extracts all the GO terms where the corresponding value is 1.\n",
    "- These GO terms are returned as a list.\n",
    "- If the protein is not found, the function returns `None`.\n",
    "\n",
    "This is particularly useful for debugging, manual verification, or evaluating the model’s output for specific proteins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5826bec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GO terms for protein P20536: ['GO:0003674', 'GO:0005488', 'GO:0005515', 'GO:0006139', 'GO:0006259', 'GO:0006725', 'GO:0006807', 'GO:0008150', 'GO:0008152', 'GO:0009058', 'GO:0009059', 'GO:0009987', 'GO:0016032', 'GO:0018130', 'GO:0019058', 'GO:0019438', 'GO:0034641', 'GO:0034654', 'GO:0043170', 'GO:0044237', 'GO:0044238', 'GO:0044249', 'GO:0044260', 'GO:0044271', 'GO:0046483', 'GO:0071704', 'GO:0090304', 'GO:1901360', 'GO:1901362', 'GO:1901576']\n"
     ]
    }
   ],
   "source": [
    "def get_go_terms(protein_id):\n",
    "    \"\"\"\n",
    "    Given a protein ID, return the corresponding GO terms.\n",
    "    \"\"\"\n",
    "    if protein_id in label_df[\"Protein Id\"].values:\n",
    "        # Get the row corresponding to the protein ID\n",
    "        row = label_df[label_df[\"Protein Id\"] == protein_id].iloc[:, 1:]\n",
    "        # Extract the GO terms where the value is 1\n",
    "        go_terms = row.columns[row.values.flatten() == 1].tolist()\n",
    "        return go_terms\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "protein_id = \"P20536\"  # Replace with a valid protein ID from your dataset\n",
    "go_terms = get_go_terms(protein_id)\n",
    "print(f\"GO terms for protein {protein_id}: {go_terms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315f828",
   "metadata": {},
   "source": [
    "### Sanity Check on the Label Matrix\n",
    "\n",
    "We randomly sample 500 proteins and verify that their GO term annotations in the label matrix (`label_df`) exactly match the original annotations from `terms_df`, after filtering to the top 1500 GO terms.\n",
    "\n",
    "For each sampled protein:\n",
    "- We extract the original terms from `terms_df`.\n",
    "- We retrieve the binarized labels from `label_df`.\n",
    "- We compare the two lists to confirm correctness.\n",
    "\n",
    "Finally, we check if all 500 comparisons are consistent. If `all(is_true)` returns `True`, we can be confident that our label matrix was constructed correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9214799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick a few proteins to manually check\n",
    "sample_ids = label_df[\"Protein Id\"].sample(500, random_state=42)\n",
    "is_true = []\n",
    "\n",
    "for pid in sample_ids:\n",
    "    original_terms = terms_df[terms_df[\"EntryID\"] == pid][\"term\"].tolist()\n",
    "    original_terms = [t for t in original_terms if t in go_terms]  # Filter to top 1500\n",
    "    predicted_terms = label_df[label_df[\"Protein Id\"] == pid].iloc[:, 1:]\n",
    "    predicted_terms = predicted_terms.loc[:, (predicted_terms == 1).any(axis=0)].columns.tolist()\n",
    "\n",
    "    #print(f\"🔍 Protein {pid}\")\n",
    "    #print(f\"From file: {sorted(original_terms)}\")\n",
    "    #print(f\"From matrix: {sorted(predicted_terms)}\")\n",
    "    #print(sorted(original_terms) == sorted(predicted_terms))\n",
    "    is_true.append(sorted(original_terms) == sorted(predicted_terms))\n",
    "    #print(\"---\")\n",
    "all(is_true)  # Check if all are True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc5a22",
   "metadata": {},
   "source": [
    "### Stratified Multi-Label Train/Validation/Test Split\n",
    "\n",
    "We use `iterative_train_test_split` from `scikit-multilearn` to perform a stratified multi-label split of our dataset, ensuring label distributions are preserved across the splits.\n",
    "\n",
    "1. First, we check that the order of protein IDs matches between the embeddings (`embedding_df`) and the label matrix (`label_df`).\n",
    "2. We extract the feature matrix `X` (protein embeddings) and label matrix `Y` (GO term assignments).\n",
    "3. We split off **10% of the data** for the test set.\n",
    "4. From the remaining 90%, we extract **~11.1%** (i.e., 1/9) as the validation set, and the rest as training.\n",
    "\n",
    "This gives us a final **80% train, 10% validation, 10% test** split, which is ideal for robust model development and fair evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5815d546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "# === Match order ===\n",
    "# Ensure alignment\n",
    "assert (embedding_df[\"Protein Id\"] == label_df[\"Protein Id\"]).all(), \"Protein ID mismatch!\"\n",
    "\n",
    "X = embedding_df.drop(\"Protein Id\", axis=1).values\n",
    "Y = label_df.drop(\"Protein Id\", axis=1).values\n",
    "protein_ids = embedding_df[\"Protein Id\"].tolist()\n",
    "\n",
    "# === Step 1: Split off 10% for test set ===\n",
    "X_remain, y_remain, X_test, y_test = iterative_train_test_split(X, Y, test_size=0.10)\n",
    "\n",
    "# === Step 2: From remaining 90%, split 8/9 for training, 1/9 for validation ===\n",
    "X_train, y_train, X_val, y_val = iterative_train_test_split(X_remain, y_remain, test_size=1/9)\n",
    "\n",
    "# === Print shapes\n",
    "print(\"✅ Final Split:\")\n",
    "print(f\"Train:      {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test:       {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d96f3f",
   "metadata": {},
   "source": [
    "###  Save Train/Validation/Test Splits\n",
    "\n",
    "We save the final train, validation, and test datasets into separate `.csv` files for future reuse and model training:\n",
    "\n",
    "- `X_train.csv`, `X_val.csv`, `X_test.csv`: contain the protein embeddings (features).\n",
    "- `y_train.csv`, `y_val.csv`, `y_test.csv`: contain the binary GO term label matrices.\n",
    "- `train_ids.csv`, `val_ids.csv`, `test_ids.csv`: store the corresponding protein IDs for each split. These are useful for tracking or merging back with original metadata if needed.\n",
    "\n",
    "This setup ensures clean modularity in our pipeline, allowing us to load only the relevant portions when training or evaluating models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb25280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "pd.DataFrame(X_train).to_csv(\"X_train.csv\", index=False)\n",
    "pd.DataFrame(X_val).to_csv(\"X_val.csv\", index=False)\n",
    "pd.DataFrame(X_test).to_csv(\"X_test.csv\", index=False)\n",
    "\n",
    "# Save labels\n",
    "pd.DataFrame(y_train, columns=label_df.columns[1:]).to_csv(\"y_train.csv\", index=False)\n",
    "pd.DataFrame(y_val, columns=label_df.columns[1:]).to_csv(\"y_val.csv\", index=False)\n",
    "pd.DataFrame(y_test, columns=label_df.columns[1:]).to_csv(\"y_test.csv\", index=False)\n",
    "\n",
    "# Optionally save protein IDs (if needed for debugging)\n",
    "train_ids = np.array(protein_ids)[np.isin(X, X_train).all(axis=1)]\n",
    "val_ids = np.array(protein_ids)[np.isin(X, X_val).all(axis=1)]\n",
    "test_ids = np.array(protein_ids)[np.isin(X, X_test).all(axis=1)]\n",
    "\n",
    "pd.DataFrame({\"Protein Id\": train_ids}).to_csv(\"train_ids.csv\", index=False)\n",
    "pd.DataFrame({\"Protein Id\": val_ids}).to_csv(\"val_ids.csv\", index=False)\n",
    "pd.DataFrame({\"Protein Id\": test_ids}).to_csv(\"test_ids.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
