{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5171d9c3",
   "metadata": {},
   "source": [
    "### Installing and Importing Required Libraries\n",
    "\n",
    "We begin by installing all the libraries needed for our protein function prediction project. This includes tools for biological data handling (`biopython`, `goatools`), machine learning (`scikit-learn`, `scikit-multilearn`), data manipulation (`pandas`, `numpy`), and visualization (`matplotlib`).\n",
    "\n",
    "After installation, we import these libraries to make them available in our environment. This setup ensures we can preprocess biological data, perform multi-label classification, manage the Gene Ontology hierarchy, and evaluate our models using metrics like F1-score and precision. We've also included utilities like `os` to handle file paths when needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac3b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Install all required libraries\n",
    "!pip install biopython\n",
    "!pip install goatools\n",
    "!pip install scikit-learn\n",
    "!pip install scikit-multilearn\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib  # Optional, for any future plotting\n",
    "\n",
    "# ✅ Now you can safely import everything used in your notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # Optional if needed\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.decomposition import PCA  # If used later\n",
    "from sklearn.metrics import f1_score, precision_score  # For evaluation\n",
    "\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "from Bio import SeqIO\n",
    "from goatools.obo_parser import GODag\n",
    "\n",
    "import os  # For managing paths if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcfab209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the embeddings from a CSV file\n",
    "embeddings = pd.read_csv('embeddings.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d8ad65",
   "metadata": {},
   "source": [
    "###  Loading Protein Sequences, Annotations, and GO Hierarchy\n",
    "\n",
    "We load three essential datasets required for our project:\n",
    "\n",
    "1. **Protein Sequences**: Using `Bio.SeqIO`, we parse the `train_sequences.fasta` file and store each protein’s ID and its amino acid sequence in a dictionary called `train_sequences`.\n",
    "\n",
    "2. **GO Annotations**: We read the `train_terms.tsv` file into a DataFrame called `train_terms`. This contains mappings between protein IDs and their associated Gene Ontology (GO) terms.\n",
    "\n",
    "3. **GO Hierarchy**: Using `goatools`, we parse the `go-basic.obo` file with `GODag`, which gives us access to the hierarchical structure of GO terms. This will be useful later when incorporating ontology-aware strategies like ancestor term propagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc9e79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\OneDrive - uniroma1.it\\Esami\\Statistical Learning\\Project\\Implementation\\Train\\go-basic.obo: fmt(1.2) rel(2023-01-01) 46,739 Terms\n"
     ]
    }
   ],
   "source": [
    "# Load train_sequences.fasta\n",
    "from Bio import SeqIO\n",
    "train_sequences = {}\n",
    "for record in SeqIO.parse(r'C:\\Users\\Utente\\OneDrive - uniroma1.it\\Esami\\Statistical Learning\\Project\\Implementation\\Train\\train_sequences.fasta', 'fasta'):\n",
    "    train_sequences[record.id] = str(record.seq)\n",
    "# Load train_terms.tsv\n",
    "train_terms = pd.read_csv(r'C:\\Users\\Utente\\OneDrive - uniroma1.it\\Esami\\Statistical Learning\\Project\\Implementation\\Train\\train_terms.tsv', sep='\\t')\n",
    "# Load go-basic.obo\n",
    "from goatools.obo_parser import GODag\n",
    "go_dag = GODag(r'C:\\Users\\Utente\\OneDrive - uniroma1.it\\Esami\\Statistical Learning\\Project\\Implementation\\Train\\go-basic.obo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32d404",
   "metadata": {},
   "source": [
    "###  Building the Target Label Matrix\n",
    "\n",
    "We construct the binary label matrix needed for multi-label classification by following these steps:\n",
    "\n",
    "1. **Top GO Terms Selection**: We focus on the 1500 most frequent GO terms across all proteins, helping reduce label sparsity and memory usage.\n",
    "\n",
    "2. **Filtering Proteins**: We ensure we only retain annotations for proteins that have corresponding embeddings available.\n",
    "\n",
    "3. **Term Grouping**: For each protein, we collect its list of GO terms and fill in empty lists for proteins with no annotations in the top 1500 terms.\n",
    "\n",
    "4. **Binarization**: Using `MultiLabelBinarizer`, we convert each protein’s list of GO terms into a binary row vector — 1 if the term is present, 0 otherwise.\n",
    "\n",
    "5. **Final Label Matrix**: We assemble everything into a clean DataFrame (`label_df`) with one row per protein and one column per GO term. This matrix is then saved for reuse.\n",
    "\n",
    "This matrix will serve as our target `Y` for training models and evaluating predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "423358df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Label matrix shape: (142246, 1501)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "k = 1500  # Number of top GO terms to keep\n",
    "\n",
    "embedding_df = pd.read_csv(\"embeddings.csv\")\n",
    "protein_ids = embedding_df[\"Protein Id\"].tolist()\n",
    "\n",
    "# === Step 2: Load GO Annotations ===\n",
    "terms_df = pd.read_csv(\"Train/train_terms.tsv\", sep='\\t')\n",
    "\n",
    "# === Step 3: Filter to proteins that have embeddings ===\n",
    "terms_df = terms_df[terms_df['EntryID'].isin(protein_ids)]\n",
    "\n",
    "# === Step 4: Keep only the top 1500 GO terms based on frequency ===\n",
    "top_terms = terms_df['term'].value_counts().nlargest(k).index\n",
    "terms_df = terms_df[terms_df['term'].isin(top_terms)]\n",
    "\n",
    "# === Step 5: Group GO terms by protein ===\n",
    "grouped_terms = terms_df.groupby(\"EntryID\")[\"term\"].apply(list)\n",
    "\n",
    "# === Step 6: Fill missing proteins with empty lists ===\n",
    "go_terms_list = [grouped_terms.get(pid, []) for pid in protein_ids]\n",
    "\n",
    "# === Step 7: Create binary matrix ===\n",
    "mlb = MultiLabelBinarizer(classes=sorted(top_terms))\n",
    "Y = mlb.fit_transform(go_terms_list)\n",
    "go_terms = mlb.classes_\n",
    "\n",
    "# === Step 8: Turn into a DataFrame ===\n",
    "label_df = pd.DataFrame(Y, columns=go_terms)\n",
    "label_df.insert(0, \"Protein Id\", protein_ids)\n",
    "\n",
    "# === (Optional) Save for reuse ===\n",
    "label_df.to_csv(\"label_matrix_1500.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Label matrix shape: {label_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f27aaf",
   "metadata": {},
   "source": [
    "###  Retrieving GO Terms for a Protein\n",
    "\n",
    "We define a helper function `get_go_terms(protein_id)` to easily retrieve the GO terms associated with a given protein:\n",
    "\n",
    "- It searches the `label_df` (our binary label matrix) for the specified `protein_id`.\n",
    "- If the protein exists, it extracts all the GO terms where the corresponding value is 1.\n",
    "- These GO terms are returned as a list.\n",
    "- If the protein is not found, the function returns `None`.\n",
    "\n",
    "This is particularly useful for debugging, manual verification, or evaluating the model’s output for specific proteins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5826bec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GO terms for protein P20536: ['GO:0003674', 'GO:0005488', 'GO:0005515', 'GO:0006139', 'GO:0006259', 'GO:0006725', 'GO:0006807', 'GO:0008150', 'GO:0008152', 'GO:0009058', 'GO:0009059', 'GO:0009987', 'GO:0016032', 'GO:0018130', 'GO:0019058', 'GO:0019438', 'GO:0034641', 'GO:0034654', 'GO:0043170', 'GO:0044237', 'GO:0044238', 'GO:0044249', 'GO:0044260', 'GO:0044271', 'GO:0046483', 'GO:0071704', 'GO:0090304', 'GO:1901360', 'GO:1901362', 'GO:1901576']\n"
     ]
    }
   ],
   "source": [
    "def get_go_terms(protein_id):\n",
    "    \"\"\"\n",
    "    Given a protein ID, return the corresponding GO terms.\n",
    "    \"\"\"\n",
    "    if protein_id in label_df[\"Protein Id\"].values:\n",
    "        # Get the row corresponding to the protein ID\n",
    "        row = label_df[label_df[\"Protein Id\"] == protein_id].iloc[:, 1:]\n",
    "        # Extract the GO terms where the value is 1\n",
    "        go_terms = row.columns[row.values.flatten() == 1].tolist()\n",
    "        return go_terms\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "protein_id = \"P20536\"  # Replace with a valid protein ID from your dataset\n",
    "go_terms = get_go_terms(protein_id)\n",
    "print(f\"GO terms for protein {protein_id}: {go_terms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315f828",
   "metadata": {},
   "source": [
    "### Sanity Check on the Label Matrix\n",
    "\n",
    "We randomly sample 500 proteins and verify that their GO term annotations in the label matrix (`label_df`) exactly match the original annotations from `terms_df`, after filtering to the top 1500 GO terms.\n",
    "\n",
    "For each sampled protein:\n",
    "- We extract the original terms from `terms_df`.\n",
    "- We retrieve the binarized labels from `label_df`.\n",
    "- We compare the two lists to confirm correctness.\n",
    "\n",
    "Finally, we check if all 500 comparisons are consistent. If `all(is_true)` returns `True`, we can be confident that our label matrix was constructed correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9214799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick a few proteins to manually check\n",
    "sample_ids = label_df[\"Protein Id\"].sample(500, random_state=42)\n",
    "is_true = []\n",
    "\n",
    "for pid in sample_ids:\n",
    "    original_terms = terms_df[terms_df[\"EntryID\"] == pid][\"term\"].tolist()\n",
    "    original_terms = [t for t in original_terms if t in go_terms]  # Filter to top 1500\n",
    "    predicted_terms = label_df[label_df[\"Protein Id\"] == pid].iloc[:, 1:]\n",
    "    predicted_terms = predicted_terms.loc[:, (predicted_terms == 1).any(axis=0)].columns.tolist()\n",
    "\n",
    "    #print(f\"🔍 Protein {pid}\")\n",
    "    #print(f\"From file: {sorted(original_terms)}\")\n",
    "    #print(f\"From matrix: {sorted(predicted_terms)}\")\n",
    "    #print(sorted(original_terms) == sorted(predicted_terms))\n",
    "    is_true.append(sorted(original_terms) == sorted(predicted_terms))\n",
    "    #print(\"---\")\n",
    "all(is_true)  # Check if all are True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe27da3",
   "metadata": {},
   "source": [
    "### Splitting the Dataset into Train, Validation, and Test Sets\n",
    "\n",
    "In this cell, we randomly split our dataset of protein embeddings and GO labels into training, validation, and test sets:\n",
    "\n",
    "1. **Extract Input and Labels**:  \n",
    "   We extract the input features `X` (embeddings), target labels `Y` (GO terms), and corresponding protein IDs from the full dataset.\n",
    "\n",
    "2. **Initial Test Split (10%)**:  \n",
    "   We use `train_test_split` to separate 10% of the data as the test set. This split is done **without stratification**.\n",
    "\n",
    "3. **Train/Validation Split (from remaining 90%)**:  \n",
    "   From the remaining data, we further split 10% for validation, resulting in approximately 80% train, 10% val, 10% test overall.\n",
    "\n",
    "4. **Check Final Shapes**:  \n",
    "   We print the final sizes of each split to confirm the partitioning.\n",
    "\n",
    "This simple approach is fast and works well when stratification is not critical or causes issues due to label imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "499e1189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final Split:\n",
      "Train:      (113796, 1024), (113796, 1500)\n",
      "Validation: (14225, 1024), (14225, 1500)\n",
      "Test:       (14225, 1024), (14225, 1500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Prepare X, Y\n",
    "X = embedding_df.drop(\"Protein Id\", axis=1).values\n",
    "Y = label_df.drop(\"Protein Id\", axis=1).values\n",
    "protein_ids = np.array(embedding_df[\"Protein Id\"])\n",
    "\n",
    "# Step 2: Simple random split (no stratify)\n",
    "X_remain, X_test, y_remain, y_test, id_remain, id_test = train_test_split(\n",
    "    X, Y, protein_ids, test_size=0.10, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Split train/val\n",
    "X_train, X_val, y_train, y_val, id_train, id_val = train_test_split(\n",
    "    X_remain, y_remain, id_remain, test_size=1/9, random_state=42\n",
    ")\n",
    "\n",
    "# Step 4: Print shapes\n",
    "print(\"✅ Final Split:\")\n",
    "print(f\"Train:      {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test:       {X_test.shape}, {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b92f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "pd.DataFrame(X_train).to_csv(\"X_train.csv\", index=False)\n",
    "pd.DataFrame(X_val).to_csv(\"X_val.csv\", index=False)\n",
    "pd.DataFrame(X_test).to_csv(\"X_test.csv\", index=False)\n",
    "\n",
    "# Save labels\n",
    "pd.DataFrame(y_train, columns=label_df.columns[1:]).to_csv(\"y_train.csv\", index=False)\n",
    "pd.DataFrame(y_val, columns=label_df.columns[1:]).to_csv(\"y_val.csv\", index=False)\n",
    "pd.DataFrame(y_test, columns=label_df.columns[1:]).to_csv(\"y_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
